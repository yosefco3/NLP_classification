{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The mission:\n#### Multi classification of news posts into categories (politics, wellness, travels etc).\n#### we will use a news data set, TF-IDF vectorization, and naive bayes model.","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import feature_extraction, model_selection, pipeline, preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import label_binarize\n\n# from imblearn.over_sampling import SMOTE\n# from imblearn.pipeline import Pipeline\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:19:21.266344Z","iopub.execute_input":"2022-04-09T16:19:21.267078Z","iopub.status.idle":"2022-04-09T16:19:22.650981Z","shell.execute_reply.started":"2022-04-09T16:19:21.267023Z","shell.execute_reply":"2022-04-09T16:19:22.650258Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\nsns.set_theme(style=\"darkgrid\")","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:19:22.652567Z","iopub.execute_input":"2022-04-09T16:19:22.652823Z","iopub.status.idle":"2022-04-09T16:19:22.661015Z","shell.execute_reply.started":"2022-04-09T16:19:22.652788Z","shell.execute_reply":"2022-04-09T16:19:22.660306Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Loading the json file**","metadata":{}},{"cell_type":"code","source":"lst_dics = []\nwith open('/kaggle/input/news-category-dataset/News_Category_Dataset_v2.json', mode='r', errors='ignore') as json_file:\n    for dic in json_file:\n        lst_dics.append( json.loads(dic) )\nlst_dics[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:19:22.953432Z","iopub.execute_input":"2022-04-09T16:19:22.954247Z","iopub.status.idle":"2022-04-09T16:19:24.824775Z","shell.execute_reply.started":"2022-04-09T16:19:22.954197Z","shell.execute_reply":"2022-04-09T16:19:24.824104Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Create pandas df and observe it:**","metadata":{}},{"cell_type":"code","source":"## create dtf\ndf = pd.DataFrame(lst_dics)\ndf.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:19:39.631077Z","iopub.execute_input":"2022-04-09T16:19:39.631350Z","iopub.status.idle":"2022-04-09T16:19:39.939751Z","shell.execute_reply.started":"2022-04-09T16:19:39.631319Z","shell.execute_reply":"2022-04-09T16:19:39.939104Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:20:02.023569Z","iopub.execute_input":"2022-04-09T16:20:02.024158Z","iopub.status.idle":"2022-04-09T16:20:02.031891Z","shell.execute_reply.started":"2022-04-09T16:20:02.024119Z","shell.execute_reply":"2022-04-09T16:20:02.031011Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:20:15.739509Z","iopub.execute_input":"2022-04-09T16:20:15.740044Z","iopub.status.idle":"2022-04-09T16:20:15.909806Z","shell.execute_reply.started":"2022-04-09T16:20:15.740009Z","shell.execute_reply":"2022-04-09T16:20:15.908147Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**we dont interested in all the columns, just in the description and headline, to create our NLP model.\nwe will concat them into a one column.\nour target is the \"category\" column:**","metadata":{}},{"cell_type":"code","source":"df = df[['category','headline','short_description']]\ndf['text'] = df['headline'] +\" \"+ df['short_description']\ndf = df[['category','text']]\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:22:13.019390Z","iopub.execute_input":"2022-04-09T16:22:13.019755Z","iopub.status.idle":"2022-04-09T16:22:13.233489Z","shell.execute_reply.started":"2022-04-09T16:22:13.019719Z","shell.execute_reply":"2022-04-09T16:22:13.232784Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**we will lower all letters in the df:**","metadata":{}},{"cell_type":"code","source":"df = df.apply(lambda x: x.str.lower().str.strip() if isinstance(x, object) else x)\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:24:14.007200Z","iopub.execute_input":"2022-04-09T16:24:14.007901Z","iopub.status.idle":"2022-04-09T16:24:14.465160Z","shell.execute_reply.started":"2022-04-09T16:24:14.007864Z","shell.execute_reply":"2022-04-09T16:24:14.464338Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df.category.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:24:20.496675Z","iopub.execute_input":"2022-04-09T16:24:20.497186Z","iopub.status.idle":"2022-04-09T16:24:20.519420Z","shell.execute_reply.started":"2022-04-09T16:24:20.497148Z","shell.execute_reply":"2022-04-09T16:24:20.518717Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**we have 41 categories in the data frame. lets visualize them:**","metadata":{}},{"cell_type":"code","source":"def bar_plot_cat(df):\n    x = pd.DataFrame(df['category'].value_counts()).reset_index()\n    x.rename(columns={'index':'category','category':\"count\"},inplace=True)\n    fig, ax = plt.subplots(1,1,figsize=(14,6),dpi=(200))\n    sns.barplot(data=x, x= 'category',y = 'count', color ='g');\n    plt.xticks(rotation = 60);\n\nbar_plot_cat(df)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:25:27.872520Z","iopub.execute_input":"2022-04-09T16:25:27.873266Z","iopub.status.idle":"2022-04-09T16:25:28.923212Z","shell.execute_reply.started":"2022-04-09T16:25:27.873226Z","shell.execute_reply":"2022-04-09T16:25:28.922591Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**we will do this project only with 6 biggest categories.**        \n**we see that we have unbalanced data.**\n\n","metadata":{}},{"cell_type":"code","source":"df_reduced = df[df['category'].isin(['politics','wellness','entertainment','travel','style & beauty','parenting'])].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:26:10.595038Z","iopub.execute_input":"2022-04-09T16:26:10.595324Z","iopub.status.idle":"2022-04-09T16:26:10.629111Z","shell.execute_reply.started":"2022-04-09T16:26:10.595268Z","shell.execute_reply":"2022-04-09T16:26:10.628420Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_reduced.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:26:12.979511Z","iopub.execute_input":"2022-04-09T16:26:12.980419Z","iopub.status.idle":"2022-04-09T16:26:12.985758Z","shell.execute_reply.started":"2022-04-09T16:26:12.980375Z","shell.execute_reply":"2022-04-09T16:26:12.985080Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"bar_plot_cat(df_reduced)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:26:16.007132Z","iopub.execute_input":"2022-04-09T16:26:16.007817Z","iopub.status.idle":"2022-04-09T16:26:16.495037Z","shell.execute_reply.started":"2022-04-09T16:26:16.007761Z","shell.execute_reply":"2022-04-09T16:26:16.494329Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**a function to pre-process the text:**","metadata":{}},{"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words(\"english\")\n\ndef utils_preprocess_text(text, flg_stemm=True, flg_lemm=True, lst_stopwords= stopwords):\n    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    ## Tokenize (convert from string to list)\n    lst_text = text.split()    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:27:10.763641Z","iopub.execute_input":"2022-04-09T16:27:10.764171Z","iopub.status.idle":"2022-04-09T16:27:10.773245Z","shell.execute_reply.started":"2022-04-09T16:27:10.764131Z","shell.execute_reply":"2022-04-09T16:27:10.772430Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_reduced[\"text_clean\"] = df_reduced[\"text\"].apply(lambda x: utils_preprocess_text(x))","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:27:14.796206Z","iopub.execute_input":"2022-04-09T16:27:14.796707Z","iopub.status.idle":"2022-04-09T16:28:18.809566Z","shell.execute_reply.started":"2022-04-09T16:27:14.796669Z","shell.execute_reply":"2022-04-09T16:28:18.808813Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df_reduced.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:28:18.811191Z","iopub.execute_input":"2022-04-09T16:28:18.811626Z","iopub.status.idle":"2022-04-09T16:28:18.822841Z","shell.execute_reply.started":"2022-04-09T16:28:18.811588Z","shell.execute_reply":"2022-04-09T16:28:18.822191Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    print('before:',df_reduced['text'].iloc[i])\n    print('after:',df_reduced['text_clean'].iloc[i])\n    print(\"------------------------------------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:28:18.824154Z","iopub.execute_input":"2022-04-09T16:28:18.824618Z","iopub.status.idle":"2022-04-09T16:28:18.835935Z","shell.execute_reply.started":"2022-04-09T16:28:18.824576Z","shell.execute_reply":"2022-04-09T16:28:18.835010Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"**## Models**","metadata":{}},{"cell_type":"markdown","source":"**TF-IDF WITH NAIVE BAYES CLASSIFIER**","metadata":{}},{"cell_type":"markdown","source":"TF-IDF is a short for term frequencyâ€“inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n![definition](https://miro.medium.com/max/1400/1*NzTKUS0puSpmopsQzt6HRw.png)\n![image](https://miro.medium.com/max/1400/1*HZvxT29V9B4HxT2wx8M4XQ.png)\nthe images are from [here.](https://medium.com/codex/document-indexing-using-tf-idf-189afd04a9fc)","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\nX = df_reduced['text_clean']\ny = df_reduced['category']\nle =  LabelEncoder()\ny = le.fit_transform(y)\nclasses = le.classes_\nclasses","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:34:55.378621Z","iopub.execute_input":"2022-04-09T16:34:55.379188Z","iopub.status.idle":"2022-04-09T16:34:55.414338Z","shell.execute_reply.started":"2022-04-09T16:34:55.379148Z","shell.execute_reply":"2022-04-09T16:34:55.413630Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:34:56.693845Z","iopub.execute_input":"2022-04-09T16:34:56.694099Z","iopub.status.idle":"2022-04-09T16:34:56.700554Z","shell.execute_reply.started":"2022-04-09T16:34:56.694070Z","shell.execute_reply":"2022-04-09T16:34:56.699629Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:34:58.386062Z","iopub.execute_input":"2022-04-09T16:34:58.386616Z","iopub.status.idle":"2022-04-09T16:34:58.405240Z","shell.execute_reply.started":"2022-04-09T16:34:58.386576Z","shell.execute_reply":"2022-04-09T16:34:58.404584Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:35:00.014332Z","iopub.execute_input":"2022-04-09T16:35:00.015008Z","iopub.status.idle":"2022-04-09T16:35:00.024049Z","shell.execute_reply.started":"2022-04-09T16:35:00.014972Z","shell.execute_reply":"2022-04-09T16:35:00.023218Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"**we will use a kind of naive bayes classifier to model our data**\nnaive bayes is commonly applied to text classification.\nwe actually use bayes theorm, to apply classification tasks, by this formula:\n\n![naive bayes](https://vlsi.eelabs.technion.ac.il/wp-content/uploads/sites/18/2021/07/NBC.jpg)","metadata":{}},{"cell_type":"code","source":"## pipeline\nclassifier = MultinomialNB()\n# we could add SMOTE to treat the imbalanced data - i saw it doesnt make here a difference.\n# smt = SMOTE(random_state=42)\n# model = Pipeline([(\"vectorizer\", vectorizer),('sm',smt),(\"classifier\", classifier)])\n\nmodel = Pipeline([(\"vectorizer\", vectorizer),(\"classifier\", classifier)])\n## train classifier\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:40:01.142169Z","iopub.execute_input":"2022-04-09T16:40:01.142879Z","iopub.status.idle":"2022-04-09T16:40:07.098010Z","shell.execute_reply.started":"2022-04-09T16:40:01.142840Z","shell.execute_reply":"2022-04-09T16:40:07.097010Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# we could see here the words count:\n# dic_vocabulary = vectorizer.vocabulary_\n# dic_vocabulary","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:40:07.099901Z","iopub.execute_input":"2022-04-09T16:40:07.101446Z","iopub.status.idle":"2022-04-09T16:40:07.105119Z","shell.execute_reply.started":"2022-04-09T16:40:07.101401Z","shell.execute_reply":"2022-04-09T16:40:07.104054Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\npredicted_prob = model.predict_proba(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:40:07.106999Z","iopub.execute_input":"2022-04-09T16:40:07.107632Z","iopub.status.idle":"2022-04-09T16:40:09.345201Z","shell.execute_reply.started":"2022-04-09T16:40:07.107588Z","shell.execute_reply":"2022-04-09T16:40:09.344426Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"accuracy = metrics.accuracy_score(y_test, y_pred)\nauc = metrics.roc_auc_score(y_test, predicted_prob, \n                            multi_class=\"ovr\")\nf1 = metrics.f1_score(y_test, y_pred,average='micro')\nprint(\"Accuracy:\",  round(accuracy,2))\nprint(\"Auc:\", round(auc,2))\nprint(\"Micro f1:\",round(f1,2))\nprint(\"Detail:\")\nprint(metrics.classification_report(y_test, y_pred))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-09T17:03:55.821913Z","iopub.execute_input":"2022-04-09T17:03:55.822165Z","iopub.status.idle":"2022-04-09T17:03:55.922648Z","shell.execute_reply.started":"2022-04-09T17:03:55.822135Z","shell.execute_reply":"2022-04-09T17:03:55.921834Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"* **Precision is a measure of the ability of a classification model to identify only the relevant datapoints, while recall is a measure of the ability of a model to find all the relevant cases within a dataset. the f1 is the harmonic mean of both.**\n* **i think that because we deal with posts which could be ambivalent, these outcomes are good even it isnt a very complicated model**\n* **the macro avg is the avg of all the \"precisions\" for example, is to calculate the precision to every class, and then get the avg.** \n* **the micro avg for precision is to add all the TP and divide it by (all TP + all FP) like calculate precision to all the classes together.** \n* **in a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance (like in our case).**\n* **we have micro f1 of 0.86, and it's good for our task**\n\n","metadata":{}},{"cell_type":"code","source":"\n## Plot confusion matrix\nfig, ax = plt.subplots(3,1,figsize=(14,27))\n\ncm = metrics.confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt=\"d\", ax=ax[0], cmap=\"YlGnBu\", \n            cbar=False);\nax[0].set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n       yticklabels=classes, title=\"Confusion matrix\")\n# plt.yticks(rotation=0);\nax[0].set_yticklabels(ax[0].get_xticklabels(),rotation = 0)\nax[0].set_title(\"Confusion matrix by units:\")\n\n\n\ncm = metrics.confusion_matrix(y_test, y_pred,normalize='pred')\nsns.heatmap(cm, annot=True, fmt=\".1%\", ax=ax[1], cmap=\"RdBu\", \n            cbar=False);\nax[1].set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n       yticklabels=classes, title=\"Confusion matrix\")\n# plt.yticks(rotation=0);\nax[1].set_yticklabels(ax[0].get_xticklabels(),rotation = 0)\nax[1].set_title(\"Confusion matrix by normalize by predictions (columns):\")\n\ncm = metrics.confusion_matrix(y_test, y_pred,normalize='true')\nsns.heatmap(cm, annot=True, fmt=\".1%\", ax=ax[2], cmap=\"RdBu\", \n            cbar=False);\nax[2].set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n       yticklabels=classes, title=\"Confusion matrix\")\n# plt.yticks(rotation=0);\nax[2].set_yticklabels(ax[1].get_xticklabels(),rotation = 0)\nax[2].set_title(\"Confusion matrix normalize by true values (rows):\")\n\n# Plot roc\n# the functions get the form of \"label binarize\" - [0,0,1,0,0], so we first \n# transform y_test and y_pred to this shape:\ny_test_bin = label_binarize(y_test, classes = le.transform(classes))\ny_pred_bin = label_binarize(y_pred, classes = le.transform(classes))\n\nfig, ax = plt.subplots(nrows=1, ncols=2,figsize=(14,6))\nfor i in range(len(classes)):\n    fpr, tpr, thresholds = metrics.roc_curve(y_test_bin[:,i],  \n                           predicted_prob[:,i])\n    ax[0].plot(fpr, tpr, lw=3, \n              label='{0} (area={1:0.2f})'.format(classes[i], \n                              metrics.auc(fpr, tpr))\n               )\nax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\nax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n          xlabel='False Positive Rate', \n          ylabel=\"True Positive Rate (Recall)\", \n          title=\"Receiver operating characteristic\")\nax[0].legend(loc=\"lower right\")\nax[0].grid(True)\n    \n## Plot precision-recall curve\nfor i in range(len(classes)):\n    precision, recall, thresholds = metrics.precision_recall_curve(\n                 y_test_bin[:,i], predicted_prob[:,i])\n    ax[1].plot(recall, precision, lw=3, \n               label='{0} (area={1:0.2f})'.format(classes[i], \n                                  metrics.auc(recall, precision))\n              )\nax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n          ylabel=\"Precision\", title=\"Precision-Recall curve\")\nax[1].legend(loc=\"best\")\nax[1].grid(True)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T16:40:09.440800Z","iopub.execute_input":"2022-04-09T16:40:09.441068Z","iopub.status.idle":"2022-04-09T16:40:11.799201Z","shell.execute_reply.started":"2022-04-09T16:40:09.441032Z","shell.execute_reply":"2022-04-09T16:40:11.798407Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n* **the general accuracy and the micro f1 are both 0.86, it's good result for this mission (because we deal with text that could belong to more than one class, and the iimplications of mistakes in this classification are not serious.**\n* **the cons of the method we have used, TF-IDF vector, is that every word gets a value that not in her context. if we want more accurate and more complex model, we should consider models like BERT or Word2Vec. but as we said, for posts calassification, 0.86 accuracy and microf1 are good result.**\n\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}